{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6h2GiCywdCi"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from seleniumwire import webdriver\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from Function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Links\n",
    "In this notebook you can find the code we used to download the urlâ€™s linked to the first 300 pages of best book ever collection from Goodreads.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkqYDui_Nkd_"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.goodreads.com/list/show/1.Best_Books_Ever?page='\n",
    "url_1 = 'https://www.goodreads.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTpKCdoowi8F",
    "outputId": "7d893a2d-4e57-4b5b-8eb5-fc95e54a4899"
   },
   "outputs": [],
   "source": [
    "f = open('links_1.txt', 'a')\n",
    "count = 100\n",
    "\n",
    "for i in tqdm(range(1, 301)):\n",
    "    try:\n",
    "        cnt = requests.get(url + str(i))\n",
    "        soup = BeautifulSoup(cnt.content, features = 'lxml')\n",
    "\n",
    "        #select all the link in a page \n",
    "        links = soup.find_all('a', class_=\"bookTitle\")\n",
    "\n",
    "        #write the link in the file \n",
    "        for link in links:\n",
    "            f.write(url_1 + link.get('href')+'\\n')\n",
    "\n",
    "        # open new file every 100 pages\n",
    "        if i == count:\n",
    "            f.close()\n",
    "            count += 100\n",
    "            f = open('links_' + str(count)[0] + '.txt', 'a')\n",
    "            \n",
    "    except Exception as e:\n",
    "\n",
    "    #saving information about errors\n",
    "    log_error = open('error_' +  str(e) + '.txt', 'a')\n",
    "    log_error.writelines('page_number:  ' + str(i))\n",
    "    log_error.close()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download HTML pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "driver.scopes = '.+goodreads.com/book/show+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('links_1.txt', 'r')\n",
    "Links = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_page = 1\n",
    "num_article = 1\n",
    "\n",
    "for i, link in enumerate(tqdm(Links)):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        resp = driver.last_request.response.status_code\n",
    "        #check page response\n",
    "        if resp == 200:\n",
    "            #file name\n",
    "            filename = \"HTML_pages_1/page_\"+'{0:03}'.format(num_page)+\"/article_\"+'{0:05}'.format(i+num_article)+\".html\"\n",
    "            # make directory\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            #open file\n",
    "            f_html = open(filename, \"a\", encoding=\"utf-8\")\n",
    "            soup = BeautifulSoup(driver.page_source, features=\"lxml\")\n",
    "            f_html.write(soup.prettify())\n",
    "            f_html.close()\n",
    "        else:\n",
    "            #saving information about bad response\n",
    "            error_name = \"Erros/bad_response_\"+str(resp)+\".txt\"\n",
    "            os.makedirs(os.path.dirname(error_name), exist_ok=True)\n",
    "            f_error = open(error_name, 'a')\n",
    "            f_error.write(link+\"\\n\")\n",
    "            f_error.close()\n",
    "        if (i+num_article)%100 == 0:\n",
    "            num_page += 1\n",
    "    except Exception as e:\n",
    "\n",
    "        #saving information about errors\n",
    "        os.makedirs(os.path.dirname('Errors_parsing'), exist_ok=True)\n",
    "        log_error = open('Errors_parsing/error_' +  str(e) + '.txt', 'a')\n",
    "        log_error.writelines(link+\"\\n\")\n",
    "        log_error.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tsv = \"Books.tsv\"\n",
    "f = open('links_1.txt', 'r', encoding= \"utf-8\")\n",
    "Links = f.readlines()\n",
    "f.close()\n",
    "\n",
    "out_file = open(file_tsv, 'a', encoding= \"utf-8\", newline='') \n",
    "tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "# Name of columns in tsv file\n",
    "tsv_writer.writerow(['bookId', 'bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', 'ratingCount', 'reviewCount', \n",
    "                     'Plot', 'NumberofPages', 'PublishingDate', 'Characters', 'Settings', 'url'])\n",
    "\n",
    "for drk in tqdm(os.listdir(\"HTML_pages\")):\n",
    "    for page in os.listdir(\"HTML_pages/\"+drk):\n",
    "        try:\n",
    "\n",
    "            file = open('HTML_pages/'+drk+'/'+page, encoding= \"utf-8\")\n",
    "            soup = BeautifulSoup(file, features = 'lxml')\n",
    "            file.close()\n",
    "\n",
    "            tsv_writer.writerow([page[8:-5]]+scrap_book(soup)+[Links[int(page[8:-5])-1]])\n",
    "\n",
    "        except Exception as e:\n",
    "            \n",
    "            #saving information about errors\n",
    "            os.makedirs('Errors', exist_ok=True)\n",
    "            log_error = open('Errors/error_' +  str(e).replace(' ', '_') + '.txt', 'a', encoding= \"utf-8\")\n",
    "            log_error.writelines(drk+'/'+page+\"\\n\")\n",
    "            log_error.close()\n",
    "\n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Download_links.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
